# Language Development Plans
tags: plan, lang-design, parsers

On one end of the spectrum, I have tools to help generate lexers/parsers, and maybe code generators:
  * extended regex with iterated capturing groups
  * tokenizer-Turing machine
  * token stream re-writer
  * expression tree data format
  * a monad that can perform general assembly:
    That is, I want to specify various alternative outputs, output literals, output (sized) blanks which can later be filled in, and depend on the offset of labeled bits of code as well as the current location

Somewhere in the middle, there is eexprs:
  * rewrite mixfixes
  * eexpr language to specify mixfixes, extracted from other eexpr streams
  * iterate the spec: simplify and add featurefulness (not necessarily features)

And at the other end, there is the actual target:
  * a HoTT-based dependent programming language
  * compiled to textbook
  * the textbook is also API docs
  * where typechecking is sound
  * and it can be compiled to native (possibly through intermediates)


## tl;dr the plan

  1. write a spec for the token stream data format, implement a Haskell codec
  2. go again at implementing mixfixes
  3. (improved by 2) use eexprs to parse a minimal "textbook language"
  4. (requires 3) compile a single "textbook lang" file to pandoc/html
  5. use yaml or toml to parse a "textbook language" package configuration file
  6. (requires 3, 4, 5) link several "textbook language" files into a textbook site
  7. (improves on 4, 6) instead of parsing a stupid textbook lang, parse a dependently-type programming language and compile to a textbook
  8. (requires 1) implement a general physical tokenizer, and a general token stream rewriter
  9. (requires 1) write a spec for abstract syntax trees and advanced abstract binding trees
  10. (requires 1, 2; improved by 8) revisit and reimplement eexprs
  11. (requires 2, 3, 6, 7, 8, 9, 10) typechecker for dep-typed-lang, which would need a reducer (interpreter)
  12. (optional; improves on 8) add more features or an advanced regex engine to the tools in 8
  13. (improves on 8, requires 1, 9) write a parser generator over token streams


So, I need to start with either:
  * a spec for token streams
  * a new mixfix implementation

I dunno yet

## Standards for Interfacing Compiler Pipeline Stages

The nice thing about developing such standards (and then sticking to them) is that I can change out the implementation of each stage independently of the others.
That is, I can make a special-purpose eexpr tokenizer, and then replace it later with a general-purpose one.

Normally, the pipeline is listed as `lexical analysis → parsing → name resolution → type checking → optimization ⮌→ code generation`, or words `frontend → optimizer ⮌→ backend`.
In fact, I think there are more things going on, even just in the frontend.
I think every language has a distinction between _physical_ and _logical_ tokens (and possibly several different notions of logical token).
The stream of physical tokens single-covers the entire input, and are very easy to match.
These are then transformed into logical tokens by such processes as:
  * merge adjacent whitespace (and whitespace-like) tokens
  * disambiguate the meaning of a token based on what is around it (e.g. a dot with whitespace around it is different from a dot without surrounding whitespace
  * detect certain errors (i.e. a number cannot immediately follow a symbol)
  * drop tokens, e.g. whitespace and comments
  * interpreting tokens into string or numerical values where appropriate
  * inserting new tokens based on context (i.e. indent and dedent)
  * …

So, we should replace `lexical analysis →` with `physical tokenization → logical tokenization ⮌→`.
To do this, I'll need two data formats:
  * one for physical tokens which gives:
    * a token type,
    * the original text matched,
    * the location in at least byte offset from start of input, but possibly filename, line/col numbers and byte offset from line as well
  * one for logical tokens, which gives:
    * a token type,
    * a synthesized text (essentially the interpretation),
    * the list of physical tokens that synthesized it,
    * possibly a pre-computed min/max location;
    * or none of the above, instead just a physical token

At first, the data formats will simply be embedded in JSON, but there's no saying it couldn't later be encoded with C structs, Haskell ADTs, s-expressions, some new, fancy format, or whatever.

## Implementing those Standards

My expectation is that Haskell will make short work of this.
However, it's not super-easy to call Haskell code form other languages;
I'd possibly prefer a Zig implementation.

However, how do I arrange a cabal project to link to Zig?
Cabal at least knows about C, but Zig is certainly unknown to it.
I can think of two possible ways:
  * if cabal offers configuration for which c compiler to use, I can set it to `zig` (seems fragile)
  * compile zig to C headers and a static library, then include those headers and link to the static lib (I at least tried something like this in eexprs, though it might have been a dynamic library)

## Applying those Standards

I could write a physical tokenizer for eexprs, then a logical token rewrite system on those physical tokens, then a parser on those logical tokens.
Then I'd have (another) eexpr parser, but with total ability to reconstruct the original input

On the other hand, what I really want is an HoTT lang, and I already have an eexpr parser.
On the gripping hand, it wouldn't take too long to at least write the _spec_, and perhaps a codec for it in Haskell.

Once I have the spec, I could start on generalized lexing tools, probably starting with existing regex tools, and moving to a custom regex implementation *only if I find it necessary*.
